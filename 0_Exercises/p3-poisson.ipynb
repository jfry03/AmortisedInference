{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson random variables with unknown rate\n",
    "\n",
    "Consider a collection of Poisson random variables, $X_1, X_2, \\ldots, X_N$, $N > 0$, with rate $\\lambda$. We write this as\n",
    "\n",
    "$$\n",
    "  (X_i \\mid \\lambda) \\sim \\text{Poisson}(\\lambda), \\quad i = 1, 2, \\ldots, N.\n",
    "$$\n",
    "\n",
    "Suppose that $\\lambda$ is unknown. In a Bayesian framework, we can assign a prior distribution to $\\lambda$. For $\\lambda$, we can choose a gamma prior distribution with shape parameter $a$ and rate parameter $b$. We write this as:\n",
    "\n",
    "$$\n",
    "  p(\\lambda) \\sim \\text{Gamma}(a, b).\n",
    "$$\n",
    "\n",
    "## Sampling from the prior and likelihood\n",
    "\n",
    "We can generate from this prior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.25164976]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = 1\n",
    "b = 5\n",
    "\n",
    "lambda_sample = np.random.gamma(a, b, size=1)\n",
    "print(lambda_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw $K$ samples using numpy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.37316324  4.14762596 17.63245861  3.81641183]\n"
     ]
    }
   ],
   "source": [
    "K = 4\n",
    "lambda_batch_example = np.random.gamma(a, b, size=K)\n",
    "print(lambda_batch_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditioned on a value of $\\lambda$, we can generate $N$ samples $x_1, x_2, \\ldots, x_N$ from their distribution (the likelihood) as follows. Equivalently we can let $\\mathbf{x} = (x_1, x_2, \\ldots, x_N)$ be a random variable drawn from the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  9  5  9  1  4  7  3  4  6  7  8  7  2  7  6  7  4  4 11  7  9  7 10\n",
      "  8  9  5  8  6  5 10  6  8  6  6  5  7  7  6  9 12  6  4  9  7  5  8  6\n",
      "  7  5  3  6  3 11  3  5  7  4  7  6  5  5  9  2]\n"
     ]
    }
   ],
   "source": [
    "N = 64\n",
    "x = np.random.poisson(lambda_sample, size=N)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a batch of $K$ values of $\\sigma_k$, $k = 1, 2, \\ldots, K$, we can generate $K$ batches of $\\mathbf{x}_k$, where each batch has standard deviation $\\sigma_k$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64)\n"
     ]
    }
   ],
   "source": [
    "x_batch_example = np.random.poisson(lambda_batch_example.reshape(-1, 1), size=(K, N))\n",
    "print(x_batch_example.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the sample is an estimator of $\\lambda$. We can compute this for each batch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means:\n",
      "[ 8.37316324  4.14762596 17.63245861  3.81641183]\n",
      "[ 8.140625  3.875    17.8125    3.8125  ]\n"
     ]
    }
   ],
   "source": [
    "x_batch_example_mean = np.mean(x_batch_example, axis=1)\n",
    "\n",
    "print(\"Means:\")\n",
    "print(lambda_batch_example)\n",
    "print(x_batch_example_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the empirical values are close to the true values.\n",
    "\n",
    "## Sampling from the posterior with Stan\n",
    "\n",
    "Now, to do posterior inference, we can find the posterior distribution of $\\lambda$ given the data $\\mathbf{x}$ as follows:\n",
    "\n",
    "$$\n",
    "  p(\\lambda \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid \\lambda) p(\\lambda)}{p(\\mathbf{x})} \\propto p(\\mathbf{x} \\mid \\lambda) p(\\lambda)\n",
    "$$\n",
    "\n",
    "This can be simplified to:\n",
    "\n",
    "$$\n",
    "  p(\\lambda \\mid \\mathbf{x})\n",
    "    \\propto\n",
    "      \\lambda^{a-1} \\exp\\left(-\\lambda b\\right) \\prod_{i=1}^N \\frac{\\lambda^{x_i}}{x_i!}\n",
    "$$\n",
    "\n",
    "We will use Stan to sample from this distribution. That will give a collection of samples $\\lambda^{[1]}, \\lambda^{[2]}, \\ldots, \\lambda^{[S]}$.\n",
    "\n",
    "**Exercise:** Implement this by generalising the stan code from Problems 1 and 2. Do the same as for the other exercises, comparing the posterior distribution to the true value for a single sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network inference\n",
    "\n",
    "We can now use a neural network to do inference instead. Let's target the posterior mean of $\\lambda$.\n",
    "\n",
    "**Exercise:** Implement this by generalising the code from Problem 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
